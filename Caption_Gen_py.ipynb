{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOqBQ9T7vsMlgOKtxZ7+At",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aparey/Guided-flow-match/blob/main/Caption_Gen_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smPR_-KoyUXM"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# ### Importing libraries\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, Blip2ForConditionalGeneration\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "# ### Helper Functions\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ### Loading the Caption Generation Model\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def create_processor_and_model():\n",
        "  processor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "  model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  model.to(device)\n",
        "  return processor, model\n",
        "\n",
        "\n",
        "# ### Loading the Train and Test CIFAR Dataset\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def get_dataset_and_loader():\n",
        "  transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "  batch_size = 4\n",
        "\n",
        "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "  return trainset, trainloader, testset, testloader\n",
        "\n",
        "\n",
        "# ### Custom Dataset for the Images with their Captions\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class CIFAR10WithCaptions(Dataset):\n",
        "    def __init__(self, cifar_dataset, captions):\n",
        "        self.cifar_dataset = cifar_dataset\n",
        "        self.captions = captions\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar_dataset[idx]\n",
        "        caption = self.captions[idx]\n",
        "        return image, label, caption\n",
        "\n",
        "\n",
        "# ### Generating captions and creating a new pickle file for the dataset\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def generate_caption_dataset(dataset, dataset_loader, processor, model):\n",
        "  captions = []\n",
        "  for images, labels in dataset_loader:\n",
        "    for image in images:\n",
        "         transform = transforms.ToPILImage()\n",
        "         PIL_image = transform(image)\n",
        "\n",
        "         question = \"Describe everything in this image\"\n",
        "         inputs = processor(PIL_image, question, return_tensors=\"pt\").to(\"cuda\").to(\"cuda\", torch.float16)\n",
        "\n",
        "         out = model.generate(**inputs)\n",
        "         captions.append(processor.decode(out[0], skip_special_tokens=True))\n",
        "  return CIFAR10WithCaptions(dataset, captions)"
      ]
    }
  ]
}